# Dataset Configuration for YunMin-EfficientData

# Cloud Storage Settings
storage:
  provider: "s3"  # Options: s3, gcs, azure
  bucket: "yunmin-data"
  region: "ap-northeast-2"
  
  # AWS S3 Configuration
  access_key: null  # Set via environment variable AWS_ACCESS_KEY_ID
  secret_key: null  # Set via environment variable AWS_SECRET_ACCESS_KEY
  
  # GCS Configuration (alternative)
  # credentials_path: "path/to/service-account.json"  # Set via GOOGLE_APPLICATION_CREDENTIALS
  
  # Azure Configuration (alternative)
  # connection_string: null  # Set via AZURE_STORAGE_CONNECTION_STRING
  # account_url: null  # Set via AZURE_STORAGE_ACCOUNT_URL

# Data Paths (Cloud Storage)
paths:
  raw_data: "s3://yunmin-data/raw/"
  dedup_ready: "s3://yunmin-data/dedup_ready/"
  deduped: "s3://yunmin-data/deduped/"
  parquet: "s3://yunmin-data/parquet/"
  
  # Local cache directory for temporary files
  local_cache: "./cache/"

# Dataset Schema
schema:
  required_columns:
    - "text"
    - "tokens"
    - "source"
    - "lang"
    - "domain"
  
  column_types:
    text: "string"
    tokens: "list[string]"
    source: "string"
    lang: "string"
    domain: "categorical"

# Domains
domains:
  - "main_data"
  - "textbook"
  - "assembly"
  - "web"
  - "social"

# Tokenization Settings
tokenization:
  tokenizer_name: "sentencepiece"
  vocab_size: 32000
  model_max_length: 2048
  special_tokens:
    - "<s>"
    - "</s>"
    - "<unk>"
    - "<pad>"

# Processing Settings
processing:
  chunk_size: 1000  # Number of samples to process at once
  max_workers: 4
  encoding: "utf-8"
  
# Validation
validation:
  sample_size: 100  # Number of samples for validation
  check_format: true
  check_encoding: true
  check_schema: true 