# â˜ï¸ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ê°€ì´ë“œ

YunMin-EfficientDataëŠ” AWS S3, Google Cloud Storage, Azure Blob Storageë¥¼ í†µí•© ì§€ì›í•˜ëŠ” í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## ğŸ¯ ì§€ì› ê¸°ëŠ¥

- **ë‹¤ì¤‘ í´ë¼ìš°ë“œ ì§€ì›**: AWS S3, GCS, Azure Blob Storage
- **í†µí•© API**: ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ëª¨ë“  í´ë¼ìš°ë“œ ì œê³µì ì§€ì›
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ íŒŒì¼ íš¨ìœ¨ì  ì²˜ë¦¬
- **ìë™ ì¸ì¦**: í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì¸ì¦
- **ì˜¤ë¥˜ ë³µêµ¬**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ìë™ ì¬ì‹œë„

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ìë™ ì„¤ì •

```bash
# Linux/macOS
./scripts/setup_cloud.sh

# Windows
.\scripts\setup_cloud.ps1
```

### 2. ìˆ˜ë™ ì„¤ì •

#### AWS S3

```bash
# AWS CLI ì„¤ì¹˜ ë° êµ¬ì„±
aws configure

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=ap-northeast-2

# ë²„í‚· ìƒì„± (ì„ íƒì‚¬í•­)
aws s3 mb s3://yunmin-data
```

#### Google Cloud Storage

```bash
# Google Cloud SDK ì„¤ì¹˜ ë° ì¸ì¦
gcloud auth login
gcloud config set project your-project-id

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ìƒì„±
gcloud iam service-accounts create yunmin-data-sa
gcloud iam service-accounts keys create gcs-key.json \
    --iam-account=yunmin-data-sa@your-project.iam.gserviceaccount.com

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export GOOGLE_APPLICATION_CREDENTIALS=./gcs-key.json

# ë²„í‚· ìƒì„± (ì„ íƒì‚¬í•­)
gsutil mb gs://yunmin-data
```

#### Azure Blob Storage

```bash
# Azure CLI ì„¤ì¹˜ ë° ë¡œê·¸ì¸
az login

# ìŠ¤í† ë¦¬ì§€ ê³„ì • ìƒì„±
az storage account create \
    --name yunmindata \
    --resource-group your-resource-group \
    --location koreacentral \
    --sku Standard_LRS

# ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
az storage account show-connection-string \
    --name yunmindata \
    --resource-group your-resource-group

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export AZURE_STORAGE_CONNECTION_STRING="your_connection_string"
```

## âš™ï¸ ì„¤ì • íŒŒì¼

`configs/dataset_config.yaml` íŒŒì¼ì—ì„œ í´ë¼ìš°ë“œ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

```yaml
# Cloud Storage Settings
storage:
  provider: "s3"  # Options: s3, gcs, azure
  bucket: "yunmin-data"
  region: "ap-northeast-2"
  
  # AWS S3 Configuration
  access_key: null  # Set via AWS_ACCESS_KEY_ID
  secret_key: null  # Set via AWS_SECRET_ACCESS_KEY
  
  # GCS Configuration (alternative)
  # credentials_path: "path/to/service-account.json"
  
  # Azure Configuration (alternative)
  # connection_string: null  # Set via AZURE_STORAGE_CONNECTION_STRING
  # account_url: null

# Data Paths (Cloud Storage)
paths:
  raw_data: "s3://yunmin-data/raw/"
  dedup_ready: "s3://yunmin-data/dedup_ready/"
  deduped: "s3://yunmin-data/deduped/"
  parquet: "s3://yunmin-data/parquet/"
  local_cache: "./cache/"  # Local cache for temporary files
```

## ğŸ“ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ êµ¬ì¡°

```
í´ë¼ìš°ë“œ ë²„í‚· (ì˜ˆ: s3://yunmin-data/)
â”œâ”€â”€ raw/                    # ì›ë³¸ JSONL ë°ì´í„°
â”‚   â”œâ”€â”€ dataset1.jsonl
â”‚   â””â”€â”€ dataset2.jsonl
â”œâ”€â”€ dedup_ready/           # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ deduped/               # ì¤‘ë³µ ì œê±° ì™„ë£Œ ë°ì´í„°
â”‚   â”œâ”€â”€ dataset1_deduped.jsonl
â”‚   â””â”€â”€ dataset1_stats.json
â”œâ”€â”€ parquet/               # Parquet í˜•ì‹ ë³€í™˜ ê²°ê³¼
â”‚   â”œâ”€â”€ dataset1.parquet
â”‚   â””â”€â”€ dataset1_benchmark.json
â”œâ”€â”€ models/                # í•™ìŠµëœ ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ lora/
â”‚   â”œâ”€â”€ diff_vectors/
â”‚   â””â”€â”€ merged/
â””â”€â”€ logs/                  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¡œê·¸
    â”œâ”€â”€ dedup_20231201.log
    â””â”€â”€ pipeline_run.log
```

## ğŸ”§ API ì‚¬ìš©ë²•

### CloudStorageManager ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from utils.cloud_storage import CloudStorageManager

# S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
storage = CloudStorageManager(
    provider='s3',
    access_key='your_key',
    secret_key='your_secret',
    region='ap-northeast-2'
)

# íŒŒì¼ ëª©ë¡ ì¡°íšŒ
files = storage.list_files('yunmin-data', prefix='raw/', suffix='.jsonl')

# JSONL íŒŒì¼ ì½ê¸° (ìŠ¤íŠ¸ë¦¬ë°)
for document in storage.read_jsonl_file('s3://yunmin-data/raw/dataset.jsonl'):
    print(document['text'])

# íŒŒì¼ ì—…ë¡œë“œ
success = storage.upload_file('./local_file.jsonl', 's3://yunmin-data/raw/uploaded.jsonl')

# í…ìŠ¤íŠ¸ íŒŒì¼ ì‘ì„±
content = "Hello, World!"
storage.write_text_file('s3://yunmin-data/test.txt', content)

# íŒŒì¼ ì¡´ì¬ í™•ì¸
exists = storage.file_exists('s3://yunmin-data/raw/dataset.jsonl')

# íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
info = storage.get_file_info('s3://yunmin-data/raw/dataset.jsonl')
print(f"File size: {info['size']} bytes")
```

### ì„¤ì • ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„±

```python
from utils.cloud_storage import get_storage_client
import yaml

# ì„¤ì • íŒŒì¼ ë¡œë“œ
with open('configs/dataset_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
storage = get_storage_client(config)

# íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©
for doc in storage.read_jsonl_file(config['paths']['raw_data'] + 'dataset.jsonl'):
    # ë¬¸ì„œ ì²˜ë¦¬
    pass
```

## ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

í´ë¼ìš°ë“œ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
./scripts/run_pipeline.sh s3://yunmin-data/raw/korean_dataset.jsonl

# Google Cloud Storage ì‚¬ìš©
./scripts/run_pipeline.sh gs://yunmin-data/raw/korean_dataset.jsonl

# Azure Blob Storage ì‚¬ìš©
./scripts/run_pipeline.sh azure://yunmin-data/raw/korean_dataset.jsonl

# ë‹¨ê³„ë³„ ì‹¤í–‰
./scripts/run_pipeline.sh --phase1-only s3://yunmin-data/raw/dataset.jsonl
./scripts/run_pipeline.sh --phase2-only s3://yunmin-data/deduped/dataset.jsonl
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ì²˜ë¦¬

```python
import concurrent.futures
from utils.cloud_storage import get_storage_client

def process_file(file_path):
    storage = get_storage_client(config)
    return storage.read_jsonl_file(file_path)

# ì—¬ëŸ¬ íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬
files = ['s3://bucket/file1.jsonl', 's3://bucket/file2.jsonl']
with concurrent.futures.ThreadPoolExecutor() as executor:
    results = list(executor.map(process_file, files))
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
# ëŒ€ìš©ëŸ‰ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
def process_large_file(storage, file_path, batch_size=1000):
    documents = []
    for i, doc in enumerate(storage.read_jsonl_file(file_path)):
        documents.append(doc)
        
        if len(documents) >= batch_size:
            yield documents
            documents = []
    
    if documents:
        yield documents

# ì‚¬ìš© ì˜ˆì‹œ
for batch in process_large_file(storage, 's3://bucket/large_file.jsonl'):
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    process_batch(batch)
```

### 3. ë¡œì»¬ ìºì‹œ í™œìš©

```python
import os
from pathlib import Path

def get_cached_file(storage, cloud_path, cache_dir='./cache'):
    """í´ë¼ìš°ë“œ íŒŒì¼ì„ ë¡œì»¬ ìºì‹œë¡œ ë‹¤ìš´ë¡œë“œ"""
    cache_file = Path(cache_dir) / Path(cloud_path).name
    
    if not cache_file.exists():
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        storage.download_file(cloud_path, str(cache_file))
    
    return str(cache_file)

# ì‚¬ìš© ì˜ˆì‹œ
local_file = get_cached_file(storage, 's3://bucket/large_dataset.jsonl')
```

## ğŸ”’ ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€

### 1. í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©

```bash
# .env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜
export AWS_ACCESS_KEY_ID=AKIA...
export AWS_SECRET_ACCESS_KEY=...
export AWS_DEFAULT_REGION=ap-northeast-2

# ì½”ë“œì—ì„œ í•˜ë“œì½”ë”© ê¸ˆì§€
# âŒ storage = CloudStorageManager('s3', access_key='AKIA123...', secret_key='abc123...')
# âœ… storage = CloudStorageManager('s3')  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
```

### 2. IAM ê¶Œí•œ ìµœì†Œí™”

AWS S3 IAM ì •ì±… ì˜ˆì‹œ:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::yunmin-data",
                "arn:aws:s3:::yunmin-data/*"
            ]
        }
    ]
}
```

### 3. ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ

```python
# VPC ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (AWS)
storage = CloudStorageManager(
    's3',
    region='ap-northeast-2',
    endpoint_url='https://vpce-123-s3.ap-northeast-2.vpce.amazonaws.com'
)
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. ë¡œê¹… ì„¤ì •

```python
import logging

# í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì‘ì—… ë¡œê¹…
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('cloud_storage')

# ì‘ì—… ë¡œê·¸ ì˜ˆì‹œ
logger.info(f"Starting upload: {local_file} -> {cloud_path}")
logger.info(f"Upload completed: {file_size} bytes in {duration:.2f}s")
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
import time
from utils.cloud_storage import get_storage_client

def upload_with_metrics(storage, local_path, cloud_path):
    start_time = time.time()
    file_size = os.path.getsize(local_path)
    
    success = storage.upload_file(local_path, cloud_path)
    
    duration = time.time() - start_time
    speed = file_size / duration / 1024 / 1024  # MB/s
    
    print(f"Upload: {success}, Size: {file_size/1024/1024:.1f}MB, Speed: {speed:.1f}MB/s")
    return success
```

## â— ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

1. **ì¸ì¦ ì‹¤íŒ¨**
   ```
   NoCredentialsError: Unable to locate credentials
   ```
   - í™˜ê²½ ë³€ìˆ˜ í™•ì¸: `echo $AWS_ACCESS_KEY_ID`
   - AWS CLI ì„¤ì •: `aws configure list`

2. **ê¶Œí•œ ë¶€ì¡±**
   ```
   AccessDenied: Access Denied
   ```
   - IAM ì •ì±… í™•ì¸
   - ë²„í‚· ì •ì±… í™•ì¸

3. **ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜**
   ```
   ConnectionError: Unable to connect
   ```
   - ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
   - ë°©í™”ë²½ ì„¤ì • í™•ì¸
   - í”„ë¡ì‹œ ì„¤ì • í™•ì¸

### ë””ë²„ê¹… ëª¨ë“œ

```python
import logging

# ìƒì„¸ ë¡œê¹… í™œì„±í™”
logging.getLogger('boto3').setLevel(logging.DEBUG)
logging.getLogger('botocore').setLevel(logging.DEBUG)

# ìš”ì²­/ì‘ë‹µ ë¡œê¹…
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
```

### ì—°ê²° í…ŒìŠ¤íŠ¸

```bash
# í´ë¼ìš°ë“œ ì—°ê²° í…ŒìŠ¤íŠ¸
python -c "
from utils.cloud_storage import get_storage_client
import yaml

with open('configs/dataset_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

client = get_storage_client(config)
print(f'âœ… {client.provider} connection successful')
"
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [AWS S3 ë¬¸ì„œ](https://docs.aws.amazon.com/s3/)
- [Google Cloud Storage ë¬¸ì„œ](https://cloud.google.com/storage/docs)
- [Azure Blob Storage ë¬¸ì„œ](https://docs.microsoft.com/en-us/azure/storage/blobs/)
- [boto3 ë¬¸ì„œ](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
- [google-cloud-storage ë¬¸ì„œ](https://cloud.google.com/python/docs/reference/storage/latest)
- [azure-storage-blob ë¬¸ì„œ](https://docs.microsoft.com/en-us/python/api/azure-storage-blob/) 