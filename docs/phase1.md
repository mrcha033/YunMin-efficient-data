# 📌 YunMin-EfficientData Phase 1: 중복 제거 실현 계획서

본 문서는 YunMin-EfficientData 프로젝트의 Phase 1인 **SlimPajama 기반 중복 제거** 작업에 대한 상세한 실현 계획을 설명합니다. 중복 제거는 전체 파이프라인의 데이터 효율성과 품질 향상을 위한 핵심 단계입니다.

---

## 🎯 목표

* 한국어 대규모 말뭉치 내 **의미 중복 또는 표면 유사한 텍스트 제거**
* 중복 제거 전후의 문서 수, 용량, 문장 다양성 정량 분석
* GPU 훈련 단계에서 중복 데이터로 인한 **자원 낭비 최소화**

---

## 🏗️ 전체 구조

```
raw_data/               # 클라우드에 저장된 원본 JSONL 파일
 ↓
dedup_ready/           # 전처리 및 포맷 정리된 파일
 ↓
minhash_lsh/           # MinHash 서명 및 LSH 인덱스
 ↓
deduped/               # 중복 제거 완료 파일
 ↓
dedup_log.csv          # 중복 제거 통계 및 로그
```

---

## 📁 1-1. 데이터 준비

### ✅ 세부 절차

1. **클라우드 경로 구성**: `s3://yunmin-data/raw_data/*.jsonl`
2. **파일 리스트 정리**: `boto3`를 이용하여 JSONL 파일 목록 수집
3. **JSON 형식 검증**: 각 줄이 JSON 형식인지 `try: json.loads(line)`으로 확인
4. **텍스트 전처리**

   * 공백 제거
   * 정규화: `unicodedata.normalize('NFKC', text)`
   * 이모지 제거(optional)
5. **문서 단위 통일**: 한 줄당 한 문서 기준 적용
6. **샘플 검수**: 처리 전후 문서 5개씩 수동 비교

### 📦 출력 결과

* `dedup_ready/*.jsonl`: 문서 단위로 정리된 텍스트 파일
* `dedup_ready/manifest.txt`: 파일명, 문서 수, 샘플 ID 기록

---

## 🧠 1-2. MinHash 기반 중복 탐지

### ✅ 핵심 알고리즘

1. **5-gram 토큰화**:

   * 형태소 분석기 기반 문장 단위 토큰화 (예: MeCab-ko, SentencePiece)
   * 슬라이딩 윈도우로 5-gram 생성

2. **MinHash 서명 생성**:

   * datasketch 라이브러리 사용
   * `MinHash(num_perm=128)` 서명 생성

3. **LSH 인덱싱**:

   * LSH Forest 사용
   * Jaccard 유사도 기준 0.8 이상일 경우 중복 후보로 간주

4. **검증**:

   * 랜덤 100개 문서 선택 → 후보군 탐색 → 수작업 비교

### 🧪 실험 로그

* LSH 삽입 속도
* 쿼리 시간
* 평균 후보 개수 (문서당)
* 유사도 상위 5쌍 출력

### 📦 출력 결과

* `minhash_lsh/index.pkl`: LSH Forest 인덱스
* `minhash_lsh/candidates.json`: 유사 문서쌍 목록

---

## 🧹 1-3. 중복 제거 수행

### ✅ 정책 및 기준

1. **문서 클러스터링**:

   * 유사도 기준 상호 연결된 문서들을 하나의 그룹으로 간주

2. **대표 문서 선택**:

   * 토큰 수가 가장 많은 문서를 대표로 선택
   * 또는 원본 수집 시간 기준 최신 문서 유지

3. **제거 방식**:

   * 그룹 내 대표 외 문서를 제거 대상 리스트에 포함
   * 원본 파일로부터 제거 대상 제외 후 재작성

### 📦 출력 결과

* `deduped/*.jsonl`: 중복 제거된 정제 텍스트
* `dedup_log.csv`: 문서 수, 중복률, 제거 수, 대표 선택 기준, 처리 시간 등 기록

---

## 📊 평가 지표 및 기준

| 항목              | 측정 방식                      |
| --------------- | -------------------------- |
| 중복 제거 비율        | (제거 문서 수 / 전체 문서 수) × 100% |
| LSH recall rate | 수작업 Ground Truth 대비 탐지 정확도 |
| 처리 시간           | 각 서브단계별 wall-clock 시간 기록   |
| 문서 다양성 개선       | 문서별 유니크 토큰 수 평균 증가 여부      |

---

## 📌 체크리스트 요약

* [ ] 원시 데이터 JSONL 유효성 검증 완료
* [ ] dedup\_ready 정제 파일 생성 및 검수
* [ ] 5-gram + MinHash 서명 생성 테스트 완료
* [ ] LSH Forest 유사 문서 탐지 정확도 검증
* [ ] 중복 문서 그룹 클러스터링 및 대표 선정 완료
* [ ] deduped 최종 파일 및 로그 저장

---

## ⏱️ 예상 소요 시간

| 작업 단계                 | 예상 소요 시간 (1M 문서 기준) |
| --------------------- | ------------------- |
| 1-1. 데이터 정제           | 약 30분 (병렬 처리 시 10분) |
| 1-2. MinHash & LSH 구축 | 약 1시간               |
| 1-3. 중복 제거 및 저장       | 약 30분               |
| **총합**                | **2시간 이내**          |

---

## 🔧 필요 라이브러리

* `datasketch`
* `mecab-ko` or `sentencepiece`
* `boto3` or `gcsfs`
* `pandas`, `json`, `tqdm`, `unicodedata`

---

## 📎 후속 연계 작업

* Parquet 포맷 변환 (Phase 2)
* 정제 데이터 기반 DEM 학습 (Phase 3)
* 최종 모델 성능 영향 분석 및 보고서 작성 (Phase 4)

---
